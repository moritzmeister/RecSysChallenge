{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content based Similarity approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.sparse as sps\n",
    "import scipy.io as io\n",
    "import time\n",
    "import json\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RecommenderSystem(object):\n",
    "    \n",
    "    def __init__(self, interactions_file = '../input/train_final.csv', \n",
    "                       target_playlists = '../input/target_playlists.csv', \n",
    "                       target_tracks = '../input/target_tracks.csv',\n",
    "                       meta_track = '../input/tracks_final.csv'):\n",
    "        # read interactions file\n",
    "        train_final = pd.read_csv(interactions_file, sep = '\\t')\n",
    "        train_final['interaction'] = 1\n",
    "        self.df_interactions = train_final.sort_values(['playlist_id', 'track_id'], ascending=[True, True])\n",
    "        self.numInteractions = train_final.shape[0]\n",
    "        print(\"Number of interactions (numInteractions): \" + str(self.numInteractions))\n",
    "        \n",
    "        # read target playlists which should receive a recommendation\n",
    "        self.df_target_playlists = pd.read_csv(target_playlists, sep = '\\t')\n",
    "        self.list_target_playlists = list(self.df_target_playlists['playlist_id'])\n",
    "        print(\"Size of df_target_playlists: \" + str(self.df_target_playlists.shape))\n",
    "        \n",
    "        # read target tracks\n",
    "        self.df_target_tracks = pd.read_csv(target_tracks, sep = '\\t')\n",
    "        self.list_target_tracks = list(self.df_target_tracks['track_id'])\n",
    "        print(\"Size of df_target_tracks file: \" + str(self.df_target_tracks.shape))\n",
    "        print(\"Size of list_target_tracks file: \" + str(len(self.df_target_tracks)))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # separate each column in list\n",
    "        playlist_id = list(self.df_interactions['playlist_id'])\n",
    "        track_id = list(self.df_interactions['track_id'])\n",
    "        interaction = list(self.df_interactions['interaction'])\n",
    "        \n",
    "        playlist_id_unique = list(set(playlist_id))\n",
    "        self.df_playlist_id_unique = pd.DataFrame(playlist_id_unique)\n",
    "        self.df_playlist_id_unique.reset_index(level=0, inplace=True)\n",
    "        self.df_playlist_id_unique.columns = ['index_playlist', 'playlist_id']\n",
    "        \n",
    "        track_id_unique = list(set(track_id))\n",
    "        self.df_track_id_unique = pd.DataFrame(track_id_unique)\n",
    "        self.df_track_id_unique.reset_index(level=0, inplace=True)\n",
    "        self.df_track_id_unique.columns = ['index_track', 'track_id']\n",
    "        print(\"Track_id translated to indexes: \")\n",
    "        print(self.df_track_id_unique.head())\n",
    "        print(\"\\n\")\n",
    "        print(\"Playlist_id translated to indexes: \")\n",
    "        print(self.df_playlist_id_unique.head())\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        self.numPlaylists = len(self.df_playlist_id_unique)\n",
    "        self.numTracks = len(self.df_track_id_unique)\n",
    "        print(\"Number of Playlists: \" + str(self.numPlaylists))\n",
    "        print(\"Number of Tracks: \" + str(self.numTracks))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        self.df_interactions = self.df_interactions.merge(self.df_playlist_id_unique, how='inner', on='playlist_id')\n",
    "        self.df_interactions = self.df_interactions.merge(self.df_track_id_unique, how='inner', on='track_id')\n",
    "        self.df_interactions = self.df_interactions.sort_values(['playlist_id', 'track_id'], ascending=[True, True])\n",
    "        print(\"Interactions-file with IDs translated to indexes (saved in df_interactions): \")\n",
    "        print(self.df_interactions.head())\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        self.list_index_playlist = np.array(self.df_interactions['index_playlist'])\n",
    "        self.list_index_track = np.array(self.df_interactions['index_track'])\n",
    "        self.list_interactions = np.array(self.df_interactions['interaction'])\n",
    "        \n",
    "        self.df_tracks = pd.read_csv(meta_track, sep = '\\t')\n",
    "        self.df_tracks = self.df_tracks.merge(self.df_track_id_unique, how='inner', on='track_id')\n",
    "        self.df_tracks['tags'] = self.df_tracks.tags.apply(json.loads)\n",
    "        print('Meta information about tracks read (df_tracks): ')\n",
    "        print(self.df_tracks.head())\n",
    "        print(self.df_tracks.shape)\n",
    "        \n",
    "    def target_structure(self):\n",
    "        # filter interaction dataframe, to retain only target playlists\n",
    "        train = self.df_interactions.merge(self.df_target_playlists, how='inner', on='playlist_id')\n",
    "        \n",
    "        # aggregate to playlist level and coerce tracks in that playlist to list\n",
    "        train_agg1 = train.groupby(by='playlist_id').track_id.apply(list).to_frame()\n",
    "        train_agg1.reset_index(level=0, inplace=True)\n",
    "        train_agg2 = train.groupby(by='playlist_id').index_track.apply(list).to_frame()\n",
    "        train_agg2.reset_index(level=0, inplace=True)\n",
    "        train_agg = train_agg1.merge(train_agg2, how='inner', on='playlist_id')\n",
    "        self.df_target = train_agg.merge(self.df_playlist_id_unique, how='inner', on='playlist_id')\n",
    "        self.df_target['recommend'] = np.empty((len(train_agg), 0)).tolist()\n",
    "        print(\"Data structure for final prediction was created (df_target): \")\n",
    "        print(self.df_target.head())\n",
    "        print(self.df_target.shape)\n",
    "        \n",
    "    def create_uim(self, sparse_mode=\"coo\", create_testset = True, split = 0.8):\n",
    "        if sparse_mode.lower() == \"coo\" or sparse_mode.lower() == \"csr\":\n",
    "            self.UIM = sps.coo_matrix((self.list_interactions, (self.list_index_playlist, self.list_index_track)))\n",
    "            if create_testset:\n",
    "                self.split_traintest(train_test_split = split)\n",
    "            if sparse_mode.lower() == \"csr\" and create_testset != True:\n",
    "                self.UIM = self.UIM.tocsr()\n",
    "            elif sparse_mode.lower() == \"csr\" and create_testset == True:\n",
    "                self.UIM = self.UIM.tocsr()\n",
    "                self.UIM_train = self.UIM_train.tocsr()\n",
    "                self.UIM_test = self.UIM_test.tocsr()\n",
    "                \n",
    "        else:\n",
    "            raise NotImplementedError('Sparse mode not implemented'.format(sparse_mode))\n",
    "            \n",
    "    def split_traintest(self, train_test_split):\n",
    "        train_mask = np.random.choice([True,False], self.numInteractions, p=[train_test_split, 1-train_test_split])\n",
    "        test_mask = np.logical_not(train_mask)\n",
    "        self.UIM_train = sps.coo_matrix((self.list_interactions[train_mask], \n",
    "                                        (self.list_index_playlist[train_mask], \n",
    "                                         self.list_index_track[train_mask])))\n",
    "        self.UIM_test = sps.coo_matrix((self.list_interactions, (self.list_index_playlist, self.list_index_track)))\n",
    "        print(\"UIM successfully created in csr format.\")\n",
    "        \n",
    "    def create_icm(self):\n",
    "        tags_list = []\n",
    "        for index, row in self.df_tracks.iterrows():\n",
    "            if len(row['tags']) != 0:\n",
    "                for i in row['tags']:\n",
    "                    tags_list.append([row['index_track'], i, 1])\n",
    "        tags_list = pd.DataFrame(tags_list)\n",
    "        tags_list.columns = ['index_track', 'tag', 'interaction']\n",
    "        track_list = list(tags_list['index_track'])\n",
    "        tag_list = list(tags_list['tag'])\n",
    "        interaction_list = list(tags_list['interaction'])\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(tag_list)\n",
    "        taglist_icm = le.transform(tag_list)\n",
    "        self.ICM = sps.coo_matrix((interaction_list, (track_list, taglist_icm)))\n",
    "        self.ICM = self.ICM.tocsr()\n",
    "        print(\"ICM successfully created in csr format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to calculate quality metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MAP(recommended_items, relevant_items):\n",
    "   \n",
    "    is_relevant = np.isin(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    # Cumulative sum: precision at 1, at 2, at 3 ...\n",
    "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "    \n",
    "    map_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    return map_score\n",
    "\n",
    "def evaluate_algorithm(URM_test, recommender_object, at=5):\n",
    "\n",
    "    cumulative_MAP = 0.0\n",
    "    \n",
    "    num_eval = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    for i, user_id in  enumerate(index_playlist_unique):\n",
    "        \n",
    "        if i % 5000 == 0:\n",
    "            print(\"User %d of %d\" % (i, len(index_playlist_unique)))\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "        relevant_items = URM_test[user_id].indices\n",
    "        \n",
    "        if len(relevant_items)>0:\n",
    "            \n",
    "            recommended_items = recommender_object.recommend(user_id, at=at)\n",
    "            num_eval+=1\n",
    "\n",
    "            cumulative_MAP += MAP(recommended_items, relevant_items)\n",
    "        \n",
    "    cumulative_MAP /= num_eval\n",
    "    \n",
    "    print(\"Recommender performance is: MAP = {:.4f}\".format(cumulative_MAP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BasicItemKNNRecommender(RecommenderSystem):\n",
    "       \n",
    "    def __str__(self):\n",
    "        return \"ItemKNN(similarity={},k={},shrinkage={})\".format(self.similarity_name, self.k, self.shrinkage)\n",
    "    \n",
    "    def fit(self, k=50, shrinkage=100, similarity='cosine'):\n",
    "        self.k = k\n",
    "        self.shrinkage = shrinkage\n",
    "        self.similarity_name = similarity\n",
    "        if similarity == 'cosine':\n",
    "            self.distance = Cosine(shrinkage=self.shrinkage)\n",
    "        elif similarity == 'pearson':\n",
    "            self.distance = Pearson(shrinkage=self.shrinkage)\n",
    "        elif similarity == 'adj-cosine':\n",
    "            self.distance = AdjustedCosine(shrinkage=self.shrinkage)\n",
    "        else:\n",
    "            raise NotImplementedError('Distance {} not implemented'.format(similarity))\n",
    "        \n",
    "        self.create_uim(sparse_mode = 'csr')\n",
    "        self.create_icm()\n",
    "        \n",
    "        # ok\n",
    "        item_weights = self.distance.compute(self.ICM)\n",
    "        \n",
    "        item_weights = check_matrix(item_weights, 'csr') # nearly 10 times faster\n",
    "        print(\"Converted to csr\")\n",
    "        \n",
    "        # for each column, keep only the top-k scored items\n",
    "        # THIS IS THE SLOW PART, FIND A BETTER SOLUTION        \n",
    "        values, rows, cols = [], [], []\n",
    "        nitems = self.UIM_train.shape[1]\n",
    "        for i in range(nitems):\n",
    "            if (i % 10000 == 0):\n",
    "                print(\"Item %d of %d\" % (i, nitems))\n",
    "                \n",
    "            this_item_weights = item_weights[i,:].toarray()[0]\n",
    "            top_k_idx = np.argsort(this_item_weights) [-self.k:]\n",
    "                        \n",
    "            values.extend(this_item_weights[top_k_idx])\n",
    "            rows.extend(np.arange(nitems)[top_k_idx])\n",
    "            cols.extend(np.ones(self.k) * i)\n",
    "        self.W_sparse = sps.csc_matrix((values, (rows, cols)), shape=(nitems, nitems), dtype=np.float32)\n",
    "\n",
    "    def recommend(self, user_id, at=5):\n",
    "        return 0\n",
    "\n",
    "def check_matrix(X, format='csc', dtype=np.float32):\n",
    "    if format == 'csc' and not isinstance(X, sps.csc_matrix):\n",
    "        return X.tocsc().astype(dtype)\n",
    "    elif format == 'csr' and not isinstance(X, sps.csr_matrix):\n",
    "        return X.tocsr().astype(dtype)\n",
    "    elif format == 'coo' and not isinstance(X, sps.coo_matrix):\n",
    "        return X.tocoo().astype(dtype)\n",
    "    elif format == 'dok' and not isinstance(X, sps.dok_matrix):\n",
    "        return X.todok().astype(dtype)\n",
    "    elif format == 'bsr' and not isinstance(X, sps.bsr_matrix):\n",
    "        return X.tobsr().astype(dtype)\n",
    "    elif format == 'dia' and not isinstance(X, sps.dia_matrix):\n",
    "        return X.todia().astype(dtype)\n",
    "    elif format == 'lil' and not isinstance(X, sps.lil_matrix):\n",
    "        return X.tolil().astype(dtype)\n",
    "    else:\n",
    "        return X.astype(dtype)\n",
    "    \n",
    "class ISimilarity(object):\n",
    "    \"\"\"Abstract interface for the similarity metrics\"\"\"\n",
    "\n",
    "    def __init__(self, shrinkage=10):\n",
    "        self.shrinkage = shrinkage\n",
    "\n",
    "    def compute(self, X):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Cosine(ISimilarity):\n",
    "    def compute(self, X):\n",
    "        # convert to csc matrix for faster column-wise operations\n",
    "        X = check_matrix(X, 'csc', dtype=np.float32)\n",
    "\n",
    "        # 1) normalize the columns in X\n",
    "        # compute the column-wise norm\n",
    "        # NOTE: this is slightly inefficient. We must copy X to compute the column norms.\n",
    "        # A faster solution is to  normalize the matrix inplace with a Cython function.\n",
    "        Xsq = X.copy()\n",
    "        Xsq.data **= 2\n",
    "        norm = np.sqrt(Xsq.sum(axis=0))\n",
    "        norm = np.asarray(norm).ravel()\n",
    "        norm += 1e-6\n",
    "        # compute the number of non-zeros in each column\n",
    "        # NOTE: this works only if X is instance of sparse.csc_matrix\n",
    "        col_nnz = np.diff(X.indptr)\n",
    "        # then normalize the values in each column\n",
    "        X.data /= np.repeat(norm, col_nnz)\n",
    "        print(\"Normalized\")\n",
    "\n",
    "        # 2) compute the cosine similarity using the dot-product\n",
    "        dist = X * X.T\n",
    "        print(\"Computed\")\n",
    "        \n",
    "        # zero out diagonal values\n",
    "        dist = dist - sps.dia_matrix((dist.diagonal()[sp.newaxis, :], [0]), shape=dist.shape)\n",
    "        print(\"Removed diagonal\")\n",
    "        \n",
    "        # and apply the shrinkage\n",
    "        if self.shrinkage > 0:\n",
    "            dist = self.apply_shrinkage(X, dist)\n",
    "            print(\"Applied shrinkage\")    \n",
    "        \n",
    "        return dist\n",
    "\n",
    "    def apply_shrinkage(self, X, dist):\n",
    "        # create an \"indicator\" version of X (i.e. replace values in X with ones)\n",
    "        X_ind = X.copy()\n",
    "        X_ind.data = np.ones_like(X_ind.data)\n",
    "        # compute the co-rated counts\n",
    "        co_counts = X_ind * X_ind.T\n",
    "        # remove the diagonal\n",
    "        co_counts = co_counts - sps.dia_matrix((co_counts.diagonal()[sp.newaxis, :], [0]), shape=co_counts.shape)\n",
    "        # compute the shrinkage factor as co_counts_ij / (co_counts_ij + shrinkage)\n",
    "        # then multiply dist with it\n",
    "        co_counts_shrink = co_counts.copy()\n",
    "        co_counts_shrink.data += self.shrinkage\n",
    "        co_counts.data /= co_counts_shrink.data\n",
    "        dist.data *= co_counts.data\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of interactions (numInteractions): 1040522\n",
      "Size of df_target_playlists: (10000, 1)\n",
      "Size of df_target_tracks file: (32195, 1)\n",
      "Size of list_target_tracks file: 32195\n",
      "\n",
      "\n",
      "Track_id translated to indexes: \n",
      "   index_track  track_id\n",
      "0            0   1048594\n",
      "1            1   2359314\n",
      "2            2   1835030\n",
      "3            3   3670041\n",
      "4            4   1048604\n",
      "\n",
      "\n",
      "Playlist_id translated to indexes: \n",
      "   index_playlist  playlist_id\n",
      "0               0     10485762\n",
      "1               1      5767174\n",
      "2               2      7077894\n",
      "3               3     11534344\n",
      "4               4      1179658\n",
      "\n",
      "\n",
      "Number of Playlists: 45649\n",
      "Number of Tracks: 99999\n",
      "\n",
      "\n",
      "Interactions-file with IDs translated to indexes (saved in df_interactions): \n",
      "     playlist_id  track_id  interaction  index_playlist  index_track\n",
      "0           7569    162463            1            2425        62358\n",
      "87          7569    421750            1            2425        60999\n",
      "116         7569    795606            1            2425         3009\n",
      "125         7569   1195736            1            2425        55563\n",
      "195         7569   2227105            1            2425        49116\n",
      "\n",
      "\n",
      "Meta information about tracks read (df_tracks): \n",
      "   track_id  artist_id  duration  playcount   album  \\\n",
      "0   2972914        144    224000       49.0     [7]   \n",
      "1   2750239        246    157000        1.0     [8]   \n",
      "2   1550729        144    217000      554.0     [9]   \n",
      "3   2169950        144    207000      200.0     [9]   \n",
      "4   1903709        144    198000        5.0  [None]   \n",
      "\n",
      "                                     tags  index_track  \n",
      "0     [54087, 1757, 1718, 116712, 189631]        33328  \n",
      "1   [189631, 3424, 177424, 46208, 205245]        48728  \n",
      "2   [54087, 109806, 46869, 183258, 54337]        93035  \n",
      "3  [54087, 70618, 207003, 109806, 116712]        26668  \n",
      "4   [54087, 81223, 116712, 215342, 71028]        25248  \n",
      "(99999, 7)\n"
     ]
    }
   ],
   "source": [
    "test = BasicItemKNNRecommender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UIM successfully created in csr format.\n",
      "ICM successfully created in csr format.\n"
     ]
    }
   ],
   "source": [
    "test.create_uim(sparse_mode = 'csr')\n",
    "test.create_icm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Computed\n",
      "Removed diagonal\n",
      "Applied shrinkage\n"
     ]
    }
   ],
   "source": [
    "distance = Cosine(shrinkage = 10).compute(test.ICM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "item_weights = check_matrix(distance, 'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "values, rows, cols = [], [], []\n",
    "nitems = test.numTracks\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99999,)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_item_weights = item_weights[i,:].toarray()[0]\n",
    "this_item_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10968, 46466, 77946, 49462, 42596, 82668, 30275, 72729, 89633,\n",
       "       56618, 30621, 42102,  5861, 66825, 15856, 36324,  8495,  9849,\n",
       "       29486, 90849, 27381, 78505, 65078, 27818, 45161, 98501, 37273,\n",
       "       53495, 67208, 27286, 29714, 56555, 37488, 56563, 99854, 56585,\n",
       "       81296,  2656, 56608, 90744, 45764, 63989, 59019, 78805, 85044,\n",
       "       40249, 50469, 93721, 42021, 35636])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_idx = np.argsort(this_item_weights) [-50:]\n",
    "top_k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "values.extend(this_item_weights[top_k_idx])\n",
    "rows.extend(np.arange(nitems)[top_k_idx])\n",
    "cols.extend(np.ones(50) * i)\n",
    "W_sparse = sps.csc_matrix((values, (rows, cols)), shape=(nitems, nitems), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.00082847819, 0.0008911246, 0.0008911246, 0.0008911246, 0.0010522697, 0.0010522697, 0.0010522697, 0.0010522697, 0.0010522697, 0.0010522697]\n",
      "[10968, 46466, 77946, 49462, 42596, 82668, 30275, 72729, 89633, 56618, 30621, 42102, 5861, 66825, 15856, 36324, 8495, 9849, 29486, 90849, 27381, 78505, 65078, 27818, 45161, 98501, 37273, 53495, 67208, 27286, 29714, 56555, 37488, 56563, 99854, 56585, 81296, 2656, 56608, 90744, 45764, 63989, 59019, 78805, 85044, 40249, 50469, 93721, 42021, 35636]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(values)\n",
    "print(rows)\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<99999x99999 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 50 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
