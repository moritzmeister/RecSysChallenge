{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.sparse as sps\n",
    "import scipy.io as io\n",
    "import time\n",
    "import json\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to save a csr sparse matrix\n",
    "def save_sparse_csr(filename,array):\n",
    "    np.savez(filename,data = array.data ,indices=array.indices,\n",
    "             indptr =array.indptr, shape=array.shape )\n",
    "\n",
    "# function to read written csr sparse matrix\n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return sps.csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                          shape = loader['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RecommenderSystem(object):\n",
    "    \n",
    "    def __init__(self, interactions_file = '../input/train_final.csv', \n",
    "                       target_playlists = '../input/target_playlists.csv', \n",
    "                       target_tracks = '../input/target_tracks.csv',\n",
    "                       meta_track = '../input/tracks_final.csv'):\n",
    "        # read interactions file\n",
    "        train_final = pd.read_csv(interactions_file, sep = '\\t')\n",
    "        train_final['interaction'] = 1.0\n",
    "        self.df_interactions = train_final.sort_values(['playlist_id', 'track_id'], ascending=[True, True])\n",
    "        self.numInteractions = train_final.shape[0]\n",
    "        print(\"Number of interactions (numInteractions): \" + str(self.numInteractions))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # separate each column in list\n",
    "        playlist_id = list(self.df_interactions['playlist_id'])\n",
    "        track_id = list(self.df_interactions['track_id'])\n",
    "        interaction = list(self.df_interactions['interaction'])\n",
    "        \n",
    "        playlist_id_unique = list(set(playlist_id))\n",
    "        self.df_playlist_id_unique = pd.DataFrame(playlist_id_unique)\n",
    "        self.df_playlist_id_unique.reset_index(level=0, inplace=True)\n",
    "        self.df_playlist_id_unique.columns = ['index_playlist', 'playlist_id']\n",
    "        \n",
    "        track_id_unique = list(set(track_id))\n",
    "        self.df_track_id_unique = pd.DataFrame(track_id_unique)\n",
    "        self.df_track_id_unique.reset_index(level=0, inplace=True)\n",
    "        self.df_track_id_unique.columns = ['index_track', 'track_id']\n",
    "        print(\"Track_id translated to indexes (df_track_id_unique): \")\n",
    "        print(self.df_track_id_unique.head())\n",
    "        print(\"\\n\")\n",
    "        print(\"Playlist_id translated to indexes (df_playlist_id_unique): \")\n",
    "        print(self.df_playlist_id_unique.head())\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # read target playlists which should receive a recommendation\n",
    "        self.df_target_playlists = pd.read_csv(target_playlists, sep = '\\t')\n",
    "        self.list_target_playlists = list(self.df_target_playlists['playlist_id'])\n",
    "        self.df_target_playlists = self.df_target_playlists.merge(self.df_playlist_id_unique, how='inner', on='playlist_id')\n",
    "        print(\"Size of df_target_playlists: \" + str(self.df_target_playlists.shape))\n",
    "        \n",
    "        # read target tracks\n",
    "        self.df_target_tracks = pd.read_csv(target_tracks, sep = '\\t')\n",
    "        self.list_target_tracks = list(self.df_target_tracks['track_id'])\n",
    "        self.df_target_tracks = self.df_target_tracks.merge(self.df_track_id_unique, how='inner', on='track_id')\n",
    "        print(\"Size of df_target_tracks file: \" + str(self.df_target_tracks.shape))\n",
    "        print(\"Size of list_target_tracks file: \" + str(len(self.df_target_tracks)))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        self.numPlaylists = len(self.df_playlist_id_unique)\n",
    "        self.numTracks = len(self.df_track_id_unique)\n",
    "        print(\"Number of Playlists: \" + str(self.numPlaylists))\n",
    "        print(\"Number of Tracks: \" + str(self.numTracks))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        self.df_interactions = self.df_interactions.merge(self.df_playlist_id_unique, how='inner', on='playlist_id')\n",
    "        self.df_interactions = self.df_interactions.merge(self.df_track_id_unique, how='inner', on='track_id')\n",
    "        self.df_interactions = self.df_interactions.sort_values(['playlist_id', 'track_id'], ascending=[True, True])\n",
    "        print(\"Interactions-file with IDs translated to indexes (saved in df_interactions): \")\n",
    "        print(self.df_interactions.head())\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        self.list_index_playlist = np.array(self.df_interactions['index_playlist'])\n",
    "        self.list_index_track = np.array(self.df_interactions['index_track'])\n",
    "        self.list_interactions = np.array(self.df_interactions['interaction'])\n",
    "        \n",
    "        self.df_tracks = pd.read_csv(meta_track, sep = '\\t')\n",
    "        self.df_tracks = self.df_tracks.merge(self.df_track_id_unique, how='inner', on='track_id')\n",
    "        self.df_tracks['tags'] = self.df_tracks.tags.apply(json.loads)\n",
    "        self.df_tracks['album'] = self.df_tracks.album.apply(lambda x: (str(x[1:-1]) + \"a\") if x != \"[None]\" and x != \"[]\" else \"-10a\")\n",
    "        print('Meta information about tracks read (df_tracks): ')\n",
    "        print(self.df_tracks.head())\n",
    "        print(self.df_tracks.shape)\n",
    "        \n",
    "    def target_structure(self):\n",
    "        # filter interaction dataframe, to retain only target playlists\n",
    "        train = self.df_interactions.merge(self.df_target_playlists, how='inner', on='playlist_id')\n",
    "        \n",
    "        # aggregate to playlist level and coerce tracks in that playlist to list\n",
    "        train_agg1 = train.groupby(by='playlist_id').track_id.apply(list).to_frame()\n",
    "        train_agg1.reset_index(level=0, inplace=True)\n",
    "        train_agg2 = train.groupby(by='playlist_id').index_track.apply(list).to_frame()\n",
    "        train_agg2.reset_index(level=0, inplace=True)\n",
    "        train_agg = train_agg1.merge(train_agg2, how='inner', on='playlist_id')\n",
    "        self.df_target = train_agg.merge(self.df_playlist_id_unique, how='inner', on='playlist_id')\n",
    "        self.df_target['recommend'] = np.empty((len(train_agg), 0)).tolist()\n",
    "        print(\"Data structure for final prediction was created (df_target): \")\n",
    "        print(self.df_target.head())\n",
    "        print(self.df_target.shape)\n",
    "        \n",
    "    def interaction_aggregation(self):\n",
    "        \n",
    "        agg1 = self.df_interactions.groupby(by='playlist_id').track_id.apply(list).to_frame()\n",
    "        agg1.reset_index(level=0, inplace=True)\n",
    "        agg2 = self.df_interactions.groupby(by='playlist_id').index_track.apply(list).to_frame()\n",
    "        agg2.reset_index(level=0, inplace=True)\n",
    "        agg3 = self.df_interactions.groupby(by='playlist_id').nunique()\n",
    "        agg3.reset_index(level=0, inplace=True)\n",
    "        agg = agg1.merge(agg2, how='inner', on='playlist_id')\n",
    "        agg = agg.merge(agg3, how='inner', on='playlist_id')\n",
    "        print(agg[:10])\n",
    "        \n",
    "    def create_uim(self, sparse_mode=\"coo\", create_testset = False, split = 0.8):\n",
    "        if sparse_mode.lower() == \"coo\" or sparse_mode.lower() == \"csr\":\n",
    "            self.UIM = sps.coo_matrix((self.list_interactions, (self.list_index_playlist, self.list_index_track)))\n",
    "            if create_testset:\n",
    "                self.split_traintest(train_test_split = split)\n",
    "            if sparse_mode.lower() == \"csr\" and create_testset != True:\n",
    "                self.UIM = self.UIM.tocsr()\n",
    "            elif sparse_mode.lower() == \"csr\" and create_testset == True:\n",
    "                self.UIM = self.UIM.tocsr()\n",
    "                self.UIM_train = self.UIM_train.tocsr()\n",
    "                self.UIM_test = self.UIM_test.tocsr()\n",
    "                \n",
    "        else:\n",
    "            raise NotImplementedError('Sparse mode not implemented'.format(sparse_mode))\n",
    "            \n",
    "    def split_traintest(self, train_test_split):\n",
    "        train_mask = np.random.choice([True,False], self.numInteractions, p=[train_test_split, 1-train_test_split])\n",
    "        test_mask = np.logical_not(train_mask)\n",
    "        self.UIM_train = sps.coo_matrix((self.list_interactions[train_mask], \n",
    "                                        (self.list_index_playlist[train_mask], \n",
    "                                         self.list_index_track[train_mask])))\n",
    "        self.UIM_test = sps.coo_matrix((self.list_interactions, (self.list_index_playlist, self.list_index_track)))\n",
    "        print(\"UIM successfully created in csr format.\")\n",
    "        \n",
    "    def create_icm(self, include_tags = True, include_album = True, include_artist = True, include_playcount = False, include_duration = False, playcount_bins = 50, duration_bins = 3):\n",
    "        tags_list = []\n",
    "        \n",
    "        if include_playcount:\n",
    "            cbf.df_tracks['playcount'].fillna(0, inplace = True)\n",
    "            cbf.df_tracks['playcount_bin'] = pd.qcut(cbf.df_tracks['playcount'], playcount_bins, duplicates = 'drop').astype('str')\n",
    "        if include_duration:\n",
    "            cbf.df_tracks['duration_bin'] = pd.qcut(cbf.df_tracks['duration'], duration_bins, duplicates = 'drop').astype('str')\n",
    "            \n",
    "        for index, row in self.df_tracks.iterrows():\n",
    "            if len(row['tags']) != 0 and include_tags:\n",
    "                for i in row['tags']:\n",
    "                    tags_list.append([row['index_track'], i, 1.0])\n",
    "            if row['album'] != \"-10a\" and include_album:\n",
    "                tags_list.append([row['index_track'], row['album'], 1])\n",
    "            if include_artist:\n",
    "                tags_list.append([row['index_track'], str(row['artist_id']) + \"b\", 1.0])\n",
    "            if include_playcount:\n",
    "                tags_list.append([row['index_track'], row['playcount_bin'] + \"x\", 1.0])\n",
    "            if include_duration and row['duration'] != -1:\n",
    "                tags_list.append([row['index_track'], row['duration_bin'] + \"z\", 1.0])\n",
    "        tags_list = pd.DataFrame(tags_list)\n",
    "        tags_list.columns = ['index_track', 'tag', 'interaction']\n",
    "        track_list = list(tags_list['index_track'])\n",
    "        tag_list = list(tags_list['tag'])\n",
    "        self.final_taglist = list(tags_list['tag'])\n",
    "        interaction_list = list(tags_list['interaction'])\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(tag_list)\n",
    "        taglist_icm = le.transform(tag_list)\n",
    "        self.ICM = sps.coo_matrix((interaction_list, (track_list, taglist_icm)))\n",
    "        self.ICM = self.ICM.tocsr()\n",
    "        # append playcount and duration\n",
    "        # if include_playcount:\n",
    "        #     self.df_tracks['playcount'].fillna(0, inplace = True)\n",
    "        #     self.ICM = sps.hstack((self.ICM, self.df_tracks[['index_track', 'playcount']].sort_values(by = 'index_track')['playcount'].values[:,None]))\n",
    "        #     if include_duration:\n",
    "        #         self.ICM = sps.hstack((self.ICM, self.df_tracks[['index_track', 'duration']].sort_values(by = 'index_track')['duration'].values[:,None]))\n",
    "        #     self.ICM = self.ICM.tocsr()\n",
    "        print(\"ICM successfully created in csr format.\")\n",
    "        \n",
    "    def td_idf(self, ICM):\n",
    "        '''Applies TD-IDF to the ICM Matrix of the RecommenderSystem Instance'''\n",
    "        \n",
    "        num_tot_items = ICM.shape[0]\n",
    "\n",
    "        # let's count how many items have a certain feature\n",
    "        items_per_feature = (ICM > 0).sum(axis=0)\n",
    "        \n",
    "        IDF = np.array(np.log(num_tot_items / items_per_feature))[0]\n",
    "        \n",
    "        print(\"Shape of IDF\")\n",
    "        print(IDF.shape)\n",
    "        \n",
    "        ICM_idf = ICM.copy()\n",
    "        # compute the number of non-zeros in each col\n",
    "        # NOTE: this works only if X is instance of sparse.csc_matrix\n",
    "        col_nnz = np.diff(check_matrix(ICM_idf, 'csc').indptr)\n",
    "        print(\"Shape of ICM_idf\")\n",
    "        print(ICM_idf.shape)\n",
    "        # then normalize the values in each col\n",
    "        ICM_idf.data *= np.repeat(IDF, col_nnz)\n",
    "        \n",
    "        return ICM_idf\n",
    "        \n",
    "    def svd(self, matrix, k = 100):\n",
    "        \n",
    "        U, s, Vt = svds(matrix, k)\n",
    "        s_diag = np.diag(s)\n",
    "        Us = np.dot(U, s_diag)\n",
    "        # return sps.csr_matrix(np.dot(Us, Vt))\n",
    "        return np.dot(Us, Vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "from cpython.array cimport array, clone\n",
    "\n",
    "import scipy.sparse as sps\n",
    "\n",
    "cdef class Cosine_Similarity:\n",
    "\n",
    "    cdef int TopK\n",
    "    cdef long n_items\n",
    "\n",
    "    # Arrays containing the sparse data\n",
    "    cdef int[:] user_to_item_row_ptr, user_to_item_cols\n",
    "    cdef int[:] item_to_user_rows, item_to_user_col_ptr\n",
    "    cdef double[:] user_to_item_data, item_to_user_data\n",
    "\n",
    "    # In case you select no TopK\n",
    "    cdef double[:,:] W_dense\n",
    "\n",
    "    \n",
    "    def __init__(self, URM, TopK = 100):\n",
    "        \"\"\"\n",
    "        Dataset must be a matrix with items as columns\n",
    "        :param dataset:\n",
    "        :param TopK:\n",
    "        \"\"\"\n",
    "\n",
    "        super(Cosine_Similarity, self).__init__()\n",
    "\n",
    "        self.n_items = URM.shape[1]\n",
    "\n",
    "        self.TopK = min(TopK, self.n_items)\n",
    "\n",
    "        URM = URM.tocsr()\n",
    "        self.user_to_item_row_ptr = URM.indptr\n",
    "        self.user_to_item_cols = URM.indices\n",
    "        self.user_to_item_data = np.array(URM.data, dtype=np.float64)\n",
    "\n",
    "        URM = URM.tocsc()\n",
    "        self.item_to_user_rows = URM.indices\n",
    "        self.item_to_user_col_ptr = URM.indptr\n",
    "        self.item_to_user_data = np.array(URM.data, dtype=np.float64)\n",
    "\n",
    "        if self.TopK == 0:\n",
    "            self.W_dense = np.zeros((self.n_items,self.n_items))\n",
    "\n",
    "\n",
    "\n",
    "    cdef int[:] getUsersThatRatedItem(self, long item_id):\n",
    "        return self.item_to_user_rows[self.item_to_user_col_ptr[item_id]:self.item_to_user_col_ptr[item_id+1]]\n",
    "\n",
    "    cdef int[:] getItemsRatedByUser(self, long user_id):\n",
    "        return self.user_to_item_cols[self.user_to_item_row_ptr[user_id]:self.user_to_item_row_ptr[user_id+1]]\n",
    "\n",
    "    \n",
    "    \n",
    "    cdef double[:] computeItemSimilarities(self, long item_id_input):\n",
    "        \"\"\"\n",
    "        For every item the cosine similarity against other items depends on whether they have users in common. \n",
    "        The more common users the higher the similarity.\n",
    "        \n",
    "        The basic implementation is:\n",
    "        - Select the first item\n",
    "        - Loop through all other items\n",
    "        -- Given the two items, get the users they have in common\n",
    "        -- Update the similarity considering all common users\n",
    "        \n",
    "        That is VERY slow due to the common user part, in which a long data structure is looped multiple times.\n",
    "        \n",
    "        A better way is to use the data structure in a different way skipping the search part, getting directly\n",
    "        the information we need.\n",
    "        \n",
    "        The implementation here used is:\n",
    "        - Select the first item\n",
    "        - Initialize a zero valued array for the similarities\n",
    "        - Get the users who rated the first item\n",
    "        - Loop through the users\n",
    "        -- Given a user, get the items he rated (second item)\n",
    "        -- Update the similarity of the items he rated\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # Create template used to initialize an array with zeros\n",
    "        # Much faster than np.zeros(self.n_items)\n",
    "        cdef array[double] template_zero = array('d')\n",
    "        cdef array[double] result = clone(template_zero, self.n_items, zero=True)\n",
    "\n",
    "\n",
    "        cdef long user_index, user_id, item_index, item_id_second\n",
    "\n",
    "        cdef int[:] users_that_rated_item = self.getUsersThatRatedItem(item_id_input)\n",
    "        cdef int[:] items_rated_by_user\n",
    "\n",
    "        cdef double rating_item_input, rating_item_second\n",
    "\n",
    "        # Get users that rated the items\n",
    "        for user_index in range(len(users_that_rated_item)):\n",
    "\n",
    "            user_id = users_that_rated_item[user_index]\n",
    "            rating_item_input = self.item_to_user_data[self.item_to_user_col_ptr[item_id_input]+user_index]\n",
    "\n",
    "            # Get all items rated by that user\n",
    "            items_rated_by_user = self.getItemsRatedByUser(user_id)\n",
    "\n",
    "            for item_index in range(len(items_rated_by_user)):\n",
    "\n",
    "                item_id_second = items_rated_by_user[item_index]\n",
    "\n",
    "                # Do not compute the similarity on the diagonal\n",
    "                if item_id_second != item_id_input:\n",
    "                    # Increment similairty\n",
    "                    rating_item_second = self.user_to_item_data[self.user_to_item_row_ptr[user_id]+item_index]\n",
    "\n",
    "                    result[item_id_second] += rating_item_input*rating_item_second\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    def compute_similarity(self):\n",
    "\n",
    "        cdef int itemIndex, innerItemIndex\n",
    "        cdef long long topKItemIndex\n",
    "\n",
    "        cdef long long[:] top_k_idx\n",
    "\n",
    "        # Declare numpy data type to use vetor indexing and simplify the topK selection code\n",
    "        cdef np.ndarray[long, ndim=1] top_k_partition, top_k_partition_sorting\n",
    "        cdef np.ndarray[np.float64_t, ndim=1] this_item_weights_np\n",
    "\n",
    "        #cdef long[:] top_k_idx\n",
    "        cdef double[:] this_item_weights\n",
    "\n",
    "        cdef long processedItems = 0\n",
    "\n",
    "        # Data structure to incrementally build sparse matrix\n",
    "        # Preinitialize max possible length\n",
    "        cdef double[:] values = np.zeros((self.n_items*self.TopK))\n",
    "        cdef int[:] rows = np.zeros((self.n_items*self.TopK,), dtype=np.int32)\n",
    "        cdef int[:] cols = np.zeros((self.n_items*self.TopK,), dtype=np.int32)\n",
    "        cdef long sparse_data_pointer = 0\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Compute all similarities for each item\n",
    "        for itemIndex in range(self.n_items):\n",
    "\n",
    "            processedItems += 1\n",
    "\n",
    "            if processedItems % 10000==0 or processedItems==self.n_items:\n",
    "\n",
    "                itemPerSec = processedItems/(time.time()-start_time)\n",
    "\n",
    "                print(\"Similarity item {} ( {:2.0f} % ), {:.2f} item/sec, required time {:.2f} min\".format(\n",
    "                    processedItems, processedItems*1.0/self.n_items*100, itemPerSec, (self.n_items-processedItems) / itemPerSec / 60))\n",
    "\n",
    "            this_item_weights = self.computeItemSimilarities(itemIndex)\n",
    "\n",
    "            if self.TopK == 0:\n",
    "\n",
    "                for innerItemIndex in range(self.n_items):\n",
    "                    self.W_dense[innerItemIndex,itemIndex] = this_item_weights[innerItemIndex]\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Sort indices and select TopK\n",
    "                # Using numpy implies some overhead, unfortunately the plain C qsort function is even slower\n",
    "                # top_k_idx = np.argsort(this_item_weights) [-self.TopK:]\n",
    "\n",
    "                # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n",
    "                # because we avoid sorting elements we already know we don't care about\n",
    "                # - Partition the data to extract the set of TopK items, this set is unsorted\n",
    "                # - Sort only the TopK items, discarding the rest\n",
    "                # - Get the original item index\n",
    "\n",
    "                this_item_weights_np = - np.array(this_item_weights)\n",
    "                \n",
    "                # Get the unordered set of topK items\n",
    "                top_k_partition = np.argpartition(this_item_weights_np, self.TopK-1)[0:self.TopK]\n",
    "                # Sort only the elements in the partition\n",
    "                top_k_partition_sorting = np.argsort(this_item_weights_np[top_k_partition])\n",
    "                # Get original index\n",
    "                top_k_idx = top_k_partition[top_k_partition_sorting]\n",
    "\n",
    "\n",
    "\n",
    "                # Incrementally build sparse matrix\n",
    "                for innerItemIndex in range(len(top_k_idx)):\n",
    "\n",
    "                    topKItemIndex = top_k_idx[innerItemIndex]\n",
    "\n",
    "                    values[sparse_data_pointer] = this_item_weights[topKItemIndex]\n",
    "                    rows[sparse_data_pointer] = topKItemIndex\n",
    "                    cols[sparse_data_pointer] = itemIndex\n",
    "\n",
    "                    sparse_data_pointer += 1\n",
    "\n",
    "\n",
    "        if self.TopK == 0:\n",
    "\n",
    "            return np.array(self.W_dense)\n",
    "\n",
    "        else:\n",
    "\n",
    "            values = np.array(values[0:sparse_data_pointer])\n",
    "            rows = np.array(rows[0:sparse_data_pointer])\n",
    "            cols = np.array(cols[0:sparse_data_pointer])\n",
    "\n",
    "            W_sparse = sps.csr_matrix((values, (rows, cols)),\n",
    "                                    shape=(self.n_items, self.n_items),\n",
    "                                    dtype=np.float32)\n",
    "\n",
    "            return W_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BasicCFKNNRecommender(RecommenderSystem):\n",
    "       \n",
    "    def __str__(self):\n",
    "        return \"ItemKNN(similarity={},k={},shrinkage={})\".format(self.similarity_name, self.k, self.shrinkage)\n",
    "    \n",
    "    def apply_shrinkage(self, X, dist):\n",
    "        # create an \"indicator\" version of X (i.e. replace values in X with ones)\n",
    "        X_ind = X.copy()\n",
    "        X_ind.data = np.ones_like(X_ind.data)\n",
    "        # compute the co-rated counts\n",
    "        co_counts = X_ind * X_ind.T\n",
    "        # remove the diagonal\n",
    "        co_counts = co_counts - sps.dia_matrix((co_counts.diagonal()[sp.newaxis, :], [0]), shape=co_counts.shape)\n",
    "        # compute the shrinkage factor as co_counts_ij / (co_counts_ij + shrinkage)\n",
    "        # then multiply dist with it\n",
    "        co_counts_shrink = co_counts.copy()\n",
    "        co_counts_shrink.data += self.shrinkage\n",
    "        co_counts.data /= co_counts_shrink.data\n",
    "        dist.data *= co_counts.data\n",
    "        return dist\n",
    "    \n",
    "    def fit(self, shrinkage=100, similarity='cosine', k=100, remove_diag = False):\n",
    "        self.shrinkage = shrinkage\n",
    "        self.similarity_name = similarity\n",
    "        self.k = k\n",
    "        self.create_uim(sparse_mode = 'csr')\n",
    "        \n",
    "        if similarity == 'cosine':\n",
    "            self.distance = Cosine_Similarity(self.UIM, k)\n",
    "        else:\n",
    "            raise NotImplementedError('Distance {} not implemented'.format(similarity))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        item_weights = self.distance.compute_similarity()\n",
    "        \n",
    "        print(\"Similarity computed in {:.2f} seconds\".format(time.time()-start_time))\n",
    "        \n",
    "        # zero out diagonal values\n",
    "        if remove_diag:\n",
    "            item_weights = item_weights - sps.dia_matrix((item_weights.diagonal()[sp.newaxis, :], [0]), shape=item_weights.shape)\n",
    "            print(\"Removed diagonal\")\n",
    "        \n",
    "        # and apply the shrinkage\n",
    "        if self.shrinkage > 0:\n",
    "            item_weights = self.apply_shrinkage(self.UIM, item_weights)\n",
    "            print(\"Applied shrinkage\") \n",
    "        \n",
    "        item_weights = check_matrix(item_weights, 'csr') # nearly 10 times faster\n",
    "        print(\"Converted to csr\")\n",
    "        \n",
    "        self.W = item_weights\n",
    "        self.UIM_estm = self.UIM.dot(self.W)\n",
    "        \n",
    "        print('UIM_estm calculated')\n",
    "\n",
    "    def recommend(self, at=5):\n",
    "        self.target_structure()\n",
    "        start_time = time.time()\n",
    "        for index, row in self.df_target.iterrows():          \n",
    "            #get row from URM_estm\n",
    "            estm = pd.DataFrame(self.UIM_estm[row['index_playlist'],:].T.toarray())\n",
    "            estm.reset_index(level=0, inplace=True)\n",
    "            estm.columns = ['index_track','pred']\n",
    "            # filter tracks which are already in the playlist, so they can't be recommended\n",
    "            estm = estm[-estm[\"index_track\"].isin(row['index_track'])]\n",
    "            # translate track index back to track_id\n",
    "            estm = estm.merge(self.df_track_id_unique, how='inner', on='index_track')\n",
    "            # filter on target track set\n",
    "            estm = estm[estm['track_id'].isin(self.list_target_tracks)]\n",
    "            estm = estm.sort_values('pred',ascending=False)\n",
    "            # print(estm)\n",
    "            count = 1\n",
    "            for index2, row2 in estm.iterrows():\n",
    "                # insert 5 top recommendations into dataframe\n",
    "                if count < (at + 1):\n",
    "                    row['recommend'].append(int(row2['track_id']))\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break\n",
    "        print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))\n",
    "        \n",
    "def check_matrix(X, format='csc', dtype=np.float32):\n",
    "    if format == 'csc' and not isinstance(X, sps.csc_matrix):\n",
    "        return X.tocsc().astype(dtype)\n",
    "    elif format == 'csr' and not isinstance(X, sps.csr_matrix):\n",
    "        return X.tocsr().astype(dtype)\n",
    "    elif format == 'coo' and not isinstance(X, sps.coo_matrix):\n",
    "        return X.tocoo().astype(dtype)\n",
    "    elif format == 'dok' and not isinstance(X, sps.dok_matrix):\n",
    "        return X.todok().astype(dtype)\n",
    "    elif format == 'bsr' and not isinstance(X, sps.bsr_matrix):\n",
    "        return X.tobsr().astype(dtype)\n",
    "    elif format == 'dia' and not isinstance(X, sps.dia_matrix):\n",
    "        return X.todia().astype(dtype)\n",
    "    elif format == 'lil' and not isinstance(X, sps.lil_matrix):\n",
    "        return X.tolil().astype(dtype)\n",
    "    else:\n",
    "        return X.astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of interactions (numInteractions): 1040522\n",
      "\n",
      "\n",
      "Track_id translated to indexes (df_track_id_unique): \n",
      "   index_track  track_id\n",
      "0            0   1048594\n",
      "1            1   2359314\n",
      "2            2   1835030\n",
      "3            3   3670041\n",
      "4            4   1048604\n",
      "\n",
      "\n",
      "Playlist_id translated to indexes (df_playlist_id_unique): \n",
      "   index_playlist  playlist_id\n",
      "0               0     10485762\n",
      "1               1      5767174\n",
      "2               2      7077894\n",
      "3               3     11534344\n",
      "4               4      1179658\n",
      "\n",
      "\n",
      "Size of df_target_playlists: (10000, 2)\n",
      "Size of df_target_tracks file: (32194, 2)\n",
      "Size of list_target_tracks file: 32194\n",
      "\n",
      "\n",
      "Number of Playlists: 45649\n",
      "Number of Tracks: 99999\n",
      "\n",
      "\n",
      "Interactions-file with IDs translated to indexes (saved in df_interactions): \n",
      "     playlist_id  track_id  interaction  index_playlist  index_track\n",
      "0           7569    162463          1.0            2425        62358\n",
      "87          7569    421750          1.0            2425        60999\n",
      "116         7569    795606          1.0            2425         3009\n",
      "125         7569   1195736          1.0            2425        55563\n",
      "195         7569   2227105          1.0            2425        49116\n",
      "\n",
      "\n",
      "Meta information about tracks read (df_tracks): \n",
      "   track_id  artist_id  duration  playcount album  \\\n",
      "0   2972914        144    224000       49.0    7a   \n",
      "1   2750239        246    157000        1.0    8a   \n",
      "2   1550729        144    217000      554.0    9a   \n",
      "3   2169950        144    207000      200.0    9a   \n",
      "4   1903709        144    198000        5.0  -10a   \n",
      "\n",
      "                                     tags  index_track  \n",
      "0     [54087, 1757, 1718, 116712, 189631]        33328  \n",
      "1   [189631, 3424, 177424, 46208, 205245]        48728  \n",
      "2   [54087, 109806, 46869, 183258, 54337]        93035  \n",
      "3  [54087, 70618, 207003, 109806, 116712]        26668  \n",
      "4   [54087, 81223, 116712, 215342, 71028]        25248  \n",
      "(99999, 7)\n"
     ]
    }
   ],
   "source": [
    "rs = BasicCFKNNRecommender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity item 10000 ( 10 % ), 1809.67 item/sec, required time 0.83 min\n",
      "Similarity item 20000 ( 20 % ), 1820.73 item/sec, required time 0.73 min\n",
      "Similarity item 30000 ( 30 % ), 1833.97 item/sec, required time 0.64 min\n",
      "Similarity item 40000 ( 40 % ), 1821.53 item/sec, required time 0.55 min\n",
      "Similarity item 50000 ( 50 % ), 1818.73 item/sec, required time 0.46 min\n",
      "Similarity item 60000 ( 60 % ), 1826.39 item/sec, required time 0.37 min\n",
      "Similarity item 70000 ( 70 % ), 1796.24 item/sec, required time 0.28 min\n",
      "Similarity item 80000 ( 80 % ), 1800.61 item/sec, required time 0.19 min\n",
      "Similarity item 90000 ( 90 % ), 1802.65 item/sec, required time 0.09 min\n",
      "Similarity item 99999 ( 100 % ), 1805.16 item/sec, required time 0.00 min\n",
      "Similarity computed in 64.05 seconds\n",
      "Converted to csr\n",
      "UIM_estm calculated\n"
     ]
    }
   ],
   "source": [
    "rs.fit(k = 200, shrinkage = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<45649x99999 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1040522 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs.UIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate full item similarity matrix from UIM\n",
    "# normalize UIM first\n",
    "from sklearn.preprocessing import normalize\n",
    "cf_UIM_norm = normalize(rs.UIM, norm='l2', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_UIM_norm[:10,:10].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs.UIM[:10,:10].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.57735026919\n"
     ]
    }
   ],
   "source": [
    "print(rs.UIM[1,:].max())\n",
    "print(cf_UIM_norm[1,:].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "S_cf = cf_UIM_norm.T.dot(cf_UIM_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_cf[:10,:10].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000069"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_cf.max()\n",
    "#print(rs.W.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs.W[:10,:10].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<99999x99999 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 60319471 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "UIM_estm = rs.UIM.dot(S_cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.10063697,  0.09161872,  0.        ,  0.        ,  0.14433757,\n",
       "          0.19692853,  0.07302967,  0.        ,  0.31310148,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.39027346,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.04287465,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.09534626,  0.        ,  0.        ],\n",
       "        [ 0.04116935,  0.74914235,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.12844645,  0.        ,  0.        ],\n",
       "        [ 0.03028913,  0.01333926,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.01446846,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.17967362,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.07302967,  0.02644429,  0.38169663,  0.02291746],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.04589008,  0.27528483,  0.        ,  0.        ,  0.        ,\n",
       "          0.26621001,  0.12973678,  0.        ,  0.47914764,  0.05372097]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UIM_estm[:10,:10].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([99599, 99508, 99411, ...,   271,    96,    63], dtype=int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UIM_estm[1,:].nonzero()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([99950, 99599, 99508, ...,   502,   357,    96], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs.UIM_estm[1,:].nonzero()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[  1.,   0.,   0.,   0.,   1.,   1.,   2.,   0.,   5.,   0.],\n",
       "        [  0.,   0.,   0.,   2.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.],\n",
       "        [  0.,  16.,   0.,   0.,   0.,   0.,   0.,   2.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   1.,   0.,   0.,   0.,   0.,   2.,   1.,   5.,   1.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   4.,   0.,   0.,   0.,   2.,   3.,   0.,   8.,   2.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs.UIM_estm[:10,:10].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rs.UIM_estm = UIM_estm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data structure for final prediction was created (df_target): \n",
      "   playlist_id                                           track_id  \\\n",
      "0         7614  [415173, 1384962, 1609224, 1614974, 1714787, 2...   \n",
      "1         7692  [88210, 266898, 280844, 302730, 384386, 551534...   \n",
      "2         7816  [126414, 245217, 513821, 611201, 767305, 84510...   \n",
      "3         8225  [13881, 261448, 311923, 500672, 676393, 906185...   \n",
      "4         8337  [451881, 1157460, 1205536, 1210884, 3131838, 3...   \n",
      "\n",
      "                                         index_track  index_playlist recommend  \n",
      "0  [58038, 27294, 13601, 15634, 53615, 16590, 739...            2447        []  \n",
      "1  [32883, 1416, 6262, 15120, 46326, 9696, 41315,...            2478        []  \n",
      "2  [47911, 94663, 96789, 32334, 93906, 21551, 971...            2519        []  \n",
      "3  [4523, 99812, 18243, 92598, 57550, 45423, 5806...            2679        []  \n",
      "4          [74517, 40765, 59706, 62219, 95577, 7177]            2715        []  \n",
      "(10000, 5)\n",
      "--- 6.214323882261912 minutes ---\n"
     ]
    }
   ],
   "source": [
    "rs.recommend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert list to string with spaces between track_ids\n",
    "rs.df_target['recommend'] = rs.df_target['recommend'].apply(lambda x: \" \".join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rename columns for submission\n",
    "final = rs.df_target[['playlist_id','recommend']]\n",
    "final.columns = ['playlist_id','track_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   playlist_id                                track_ids\n",
      "0         7614  1401050 2556767 2828149 2510128 3319136\n",
      "1         7692  2537327 2289940 2610496 3545635 3422265\n",
      "2         7816   2762406 2097922 2542321 544173 1882174\n",
      "3         8225   2579225 2828149 2141167 112767 3687848\n",
      "4         8337   1156143 2866519 3615514 3167821 656735\n"
     ]
    }
   ],
   "source": [
    "print(final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# export file\n",
    "final.to_csv('../submission/008_cf_kFULL.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
